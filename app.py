import streamlit as st
import pandas as pd
import numpy as np
from tensorflow.keras.models import load_model
from crew_setup import crew

# ===========================================
# ğŸ§  Load Trained Autoencoder Model
# ===========================================
autoencoder = load_model("ai_sentinel_autoencoder.h5", compile=False)

# ===========================================
# ğŸ·ï¸ App Title
# ===========================================
st.title("ğŸ›¡ï¸ AI Sentinel - Log Anomaly Detector")
st.caption("A multi-agent anomaly detection system with lightweight guardrails.")

# ===========================================
# ğŸ“‚ File Upload Section
# ===========================================
uploaded_files = st.file_uploader(
    "Upload preprocessed log files (.csv)",
    type="csv",
    accept_multiple_files=True
)

# ===========================================
# ğŸ§© Lightweight Guardrail & Validation Utils
# ===========================================
def clean_text(text: str) -> str:
    """Minimal guardrail to trim irrelevant or unsafe outputs."""
    if not isinstance(text, str):
        return "Invalid model response."
    # Keep natural reasoning flow but trim extreme or irrelevant outputs
    blocked_terms = ["<html>", "</script>", "import ", "Traceback (most recent call)"]
    if any(term.lower() in text.lower() for term in blocked_terms):
        return "Response skipped due to invalid output."
    # Trim overly long responses for clarity
    return text.strip()[:800]

def validate_data(df: pd.DataFrame) -> pd.DataFrame:
    """Ensures uploaded dataset is clean and numeric."""
    if df.isnull().sum().sum() > 0:
        st.warning("âš ï¸ Missing values detected â€” replaced with 0.")
        df = df.fillna(0)
    non_numeric = [col for col in df.columns if not np.issubdtype(df[col].dtype, np.number)]
    if non_numeric:
        st.warning(f"âš ï¸ Non-numeric columns detected and ignored: {non_numeric}")
        df = df.select_dtypes(include=[np.number])
    return df

# ===========================================
# ğŸš¨ Severity Mapping Function
# ===========================================
def map_severity(error, threshold=20):
    if error > threshold * 2:
        return "Critical"
    elif error > threshold * 1.5:
        return "Major"
    else:
        return "Minor"

# ===========================================
# ğŸš€ Main Execution
# ===========================================
if uploaded_files:
    dfs = [pd.read_csv(file) for file in uploaded_files]
    df = pd.concat(dfs, ignore_index=True)
    df = validate_data(df)
    st.write("âœ… Uploaded Data Sample:")
    st.dataframe(df.head())

    # Ensure BlockId exists
    if "BlockId" not in df.columns:
        df.insert(0, "BlockId", range(1, len(df) + 1))

    # Convert to NumPy
    X = df.drop(columns=["BlockId"]).values

    # Predict reconstruction
    reconstructed = autoencoder.predict(X)
    errors = np.mean((X - reconstructed) ** 2, axis=1)

    # Threshold
    threshold = 20
    predictions = (errors > threshold).astype(int)

    # Add results
    df["Reconstruction Error"] = errors
    df["Prediction"] = ["Anomaly" if p == 1 else "Normal" for p in predictions]
    df["Severity"] = df["Reconstruction Error"].apply(lambda e: map_severity(e, threshold))

    # Summary
    st.subheader("ğŸ“Š Summary")
    st.write(f"**Total Logs:** {len(df)}")
    st.write(f"**Anomalies:** {(df['Prediction'] == 'Anomaly').sum()}")
    st.write(f"**Normal Entries:** {(df['Prediction'] == 'Normal').sum()}")

    # Preview results
    st.subheader("ğŸ” Results Preview")
    st.dataframe(df.head(20))

    # ===========================================
    # ğŸ¤– CrewAI Multi-Agent Section
    # ===========================================
    st.subheader("ğŸ§  CrewAI Agent Reasoning")
    anomaly_rows = df[df["Prediction"] == "Anomaly"]

    if not anomaly_rows.empty:
        explanations = []

        # Pick one anomaly from each severity level
        for severity_level in ["Critical", "Major", "Minor"]:
            sample = anomaly_rows[anomaly_rows["Severity"] == severity_level].head(1)

            if not sample.empty:
                sample = sample.iloc[0]
                block_id = sample["BlockId"]
                error = sample["Reconstruction Error"]
                sequence = sample.drop(["Reconstruction Error", "Prediction", "Severity"]).tolist()

                try:
                    result = crew.kickoff(inputs={
                        "block_id": str(block_id),
                        "error": float(error),
                        "sequence": [str(s) for s in sequence]
                    })

                    reason, action = "N/A", "N/A"

                    if hasattr(result, "tasks_output") and result.tasks_output:
                        outputs = result.tasks_output
                        if len(outputs) > 0:
                            reason = clean_text(outputs[0].raw)
                        if len(outputs) > 2:
                            action = clean_text(outputs[2].raw)

                except Exception as e:
                    reason = f"âš ï¸ CrewAI Error: {str(e)[:200]}"
                    action = "N/A"

                explanations.append({
                    "BlockId": block_id,
                    "Reason": reason,
                    "Severity": severity_level,
                    "Suggested Action": action
                })

        # Display CrewAI results
        final_df = pd.DataFrame(explanations)
        st.write("### ğŸ“ Example Results (1 per Severity)")
        st.dataframe(final_df)

        with st.expander("ğŸ” Full Anomaly List with Severity Buckets"):
            st.dataframe(anomaly_rows)

    else:
        st.success("âœ… No anomalies detected to analyze.")

    # ===========================================
    # ğŸ“¥ Download Results
    # ===========================================
    csv = df.to_csv(index=False).encode("utf-8")
    st.download_button("ğŸ“¥ Download Results", csv, "results.csv", "text/csv")

else:
    st.info("ğŸ‘† Please upload one or more log CSV files to start the analysis.")
